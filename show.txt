commit f2012e46ce0798811c54e6f0b7833637ada9e339
Author: JinyongPark <jinyong.park@csi.skku.edu>
Date:   Mon Jul 4 16:08:29 2022 +0900

    0704

diff --git a/README_jyp b/README_jyp
index bd2c3019d..34f643b62 100644
--- a/README_jyp
+++ b/README_jyp
@@ -4,51 +4,51 @@ am start app
 echo 1 > /proc/sys/kernel/app_switch_fin
 play app ~~
 
-
 **You should handle foreground==0 after real workload
 
-
-
-
 // Optimization plan
 
-- PTE walk (cold page sender)
+- PTE walk (cold page sender) DONE!
 send to nbd right after it is COUNTER>THRESHOLD ==> Pte walk 1/2
 
 - PTE walk (send_target_page)  DONE!
-add one bit in the entry, and set if the page is swapped (very cheap operation) 
+add one bit in the entry, and set if the page is swapped in ZRAM (very cheap operation) 
 
 - SWAP TRACE TABLE WALK 
 current implementation -> periodically walk and find (tgid, vma) which in ZRAM. and send to NBD immediately
 it needs periodical random page table walk. any better IDEA?
-1. write pte that It has been targetted. and If it is
+1. write pte that It has been targetted. and If it is to be swapped out, then swap it to NBD immediately. But nee to modify pte. So current system is good if that overhead is negligible
 
-- STOP sending / during switch_start
+- STOP sending / during switch_start      Partially Done!
 It SHOULD be implemented to maximize prefetching --> made roughly, so need to be optimized
 
 - SWAP TRACE TABLE ALLOCATION
 current implementaion -> static big table
-optimization plan     -> at first, write to static big table, and then dynamically alloc the per-app tablie
+optimization plan     -> at first, write to static big table, and then dynamically alloc the per-app table
                       -> OR, alloc application well considering offline profiling, and dynamically resizing them if it is full
 
 - CPU Binding!!
-using CPU bind of Prefetching with different core 
+using CPU bind of Prefetching with different core (current is only 1, but should increase the number of core) 
 
 - try_to_free_swap()
 current implementation -> called on critical path
-optimization plan      -> called after switch
+optimization plan      -> called after switch // it can be done in miss page handler! for example) if!switch_on-->try_to_free_swap. Easy work.
 
-- PREFETCH MISS Pages handling
-current implementaion  -> Nothing. So it could be kept in Remote if swapcache flushed
-optimization plan      -> Force them faulted after switch and MADVISE them (MADV_PAGEOUT) then it can be in ZRAM...
+- PREFETCH MISS Pages handling      Partialy Done!
+Past implementaion  -> Nothing. So it could be kept in Remote if swapcache flushed
+Current             -> Force them faulted after switch ((and MADVISE them (MADV_PAGEOUT) then it can be in ZRAM soon...))
 
-- IF target page is sent, and faulted in background
-It should not be sent again, because it might be faulted again. It is solved because once the swapped flag set, It will never be sent to remote again.
+- After target page sent, It is faulted in background         DONE!
+It should not be sent again, because it might be faulted again. It is alrealy solved because once the swapped flag set, It will never be sent to remote again.
 
 - delete swap cache
-HOW about using try_to_free_swap()
+HOW about using try_to_free_swap(). similar maybe..?
+
+- ZRAM Pressure
+If ZRAM Pressure get higher, how about COLD_THRESHOLD dec, and target percentage inc
+and if ZRAM get full, Some pages can be swapped out to NBD. We should handle this page.
+
 
-	
 // Implementation idea
 
 Should handle if the process exit--> STT free or STT clean
diff --git a/adb_data_backup/data/nbdroid/nbd_ext4.sh b/adb_data_backup/data/nbdroid/nbd_ext4.sh
index 33173a1ee..6991a6c1f 100644
--- a/adb_data_backup/data/nbdroid/nbd_ext4.sh
+++ b/adb_data_backup/data/nbdroid/nbd_ext4.sh
@@ -15,7 +15,7 @@ setenforce 0
 
 sleep 1
 
-/data/nbdroid/nbd-client 192.168.0.4 -N export1 /dev/nbd0
+/data/nbdroid/nbd-client 192.168.0.4 -N export3 /dev/nbd0
 
 sleep 1
 
diff --git a/msm-google/arch/arm64/include/asm/pgtable.h b/msm-google/arch/arm64/include/asm/pgtable.h
index e3bf8a4b2..a54391c8b 100644
--- a/msm-google/arch/arm64/include/asm/pgtable.h
+++ b/msm-google/arch/arm64/include/asm/pgtable.h
@@ -717,7 +717,8 @@ extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
  *	bits 2-7:	swap type
  *	bits 8-57:	swap offset
  *	bit  58:	PTE_PROT_NONE (must be zero)
- *	bits 59-63: swap counter
+ *	bits 59-62: swap counter (ZRAM), swap id (NBD)
+ *	bit  63: swap excepted (ZRAM swap only)
  */
 #define __SWP_TYPE_SHIFT	2
 #define __SWP_TYPE_BITS		6
@@ -733,12 +734,19 @@ extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
 #define __pte_to_swp_entry(pte)	((swp_entry_t) { pte_val(pte) })
 #define __swp_entry_to_pte(swp)	((pte_t) { (swp).val })
 
-#define __SWP_COUNTER_BITS 5
+#define __SWP_COUNTER_BITS 4
 #define __SWP_COUNTER_SHIFT	(__SWP_OFFSET_BITS + __SWP_OFFSET_SHIFT + 1)
 #define __SWP_COUNTER_MASK	((1UL << __SWP_COUNTER_BITS) - 1)
 #define __swp_counter(x)	(((x).val >> __SWP_COUNTER_SHIFT) & __SWP_COUNTER_MASK)
 #define __swp_entry_with_counter(type,offset,counter) ((swp_entry_t) { ((type) << __SWP_TYPE_SHIFT) | ((offset) << __SWP_OFFSET_SHIFT) |  ((counter) << __SWP_COUNTER_SHIFT)})
 
+#define __SWP_EXCEPTED_BIT 1
+#define __SWP_EXCEPTED_SHIFT (__SWP_COUNTER_BITS + __SWP_COUNTER_SHIFT)
+#define __SWP_EXCEPTED_MASK  ((1UL << __SWP_EXCEPTED_BIT) - 1)
+#define __swp_excepted(x)    (((x).val >> __SWP_EXCEPTED_SHIFT) & __SWP_EXCEPTED_MASK)
+#define __swp_entry_with_excepted(type,offset,counter,excepted) ((swp_entry_t) { ((type) << __SWP_TYPE_SHIFT) | ((offset) << __SWP_OFFSET_SHIFT) |  ((counter) << __SWP_COUNTER_SHIFT)   |  ((excepted) << __SWP_EXCEPTED_SHIFT)  })
+
+
 /*
  * Ensure that there are not more swap files than can be encoded in the kernel
  * PTEs.
diff --git a/msm-google/include/linux/app_aware.h b/msm-google/include/linux/app_aware.h
index ada2d69ac..47397f385 100644
--- a/msm-google/include/linux/app_aware.h
+++ b/msm-google/include/linux/app_aware.h
@@ -8,7 +8,7 @@
 
 #define ZRAM_TYPE   0
 #define NBD_TYPE    1
-#define COLD_PAGE_THRESHOLD 5
+#define COLD_PAGE_THRESHOLD 3
 #define NUM_STT_ENTRIES 20000
 
 /******************************
@@ -66,6 +66,8 @@ extern struct per_app_swap_trace *past[9];
 extern atomic_t sent_cold_page;
 extern atomic_t faulted_cold_page;
 
+extern atomic_t excepted_page;
+
 
 extern int swapcache_flush;
 extern int swapcache_flush_handler(struct ctl_table *table, int write,
@@ -120,7 +122,6 @@ extern int swapin_vma_tracking;
 extern int swapin_anon_tracking;
 extern int prefetch_on;
 extern int target_percentage;
-extern int random_nbd_entry;
 
 
 
diff --git a/msm-google/include/linux/page-flags.h b/msm-google/include/linux/page-flags.h
index 5de46a658..935c91a94 100644
--- a/msm-google/include/linux/page-flags.h
+++ b/msm-google/include/linux/page-flags.h
@@ -106,6 +106,9 @@ enum pageflags {
 #if defined(CONFIG_IDLE_PAGE_TRACKING) && defined(CONFIG_64BIT)
 	PG_young,
 	PG_idle,
+#endif
+#ifdef CONFIG_APP_AWARE
+	PG_excepted,
 #endif
 	__NR_PAGEFLAGS,
 
@@ -377,6 +380,12 @@ TESTCLEARFLAG(Young, young, PF_ANY)
 PAGEFLAG(Idle, idle, PF_ANY)
 #endif
 
+
+#ifdef CONFIG_APP_AWARE
+PAGEFLAG(Excepted, excepted, PF_ANY)
+#endif
+
+
 /*
  * On an anonymous page mapped into a user virtual memory area,
  * page->mapping points to its anon_vma, not to a struct address_space;
diff --git a/msm-google/include/linux/swapops.h b/msm-google/include/linux/swapops.h
index 2c16f42b9..021c1e283 100644
--- a/msm-google/include/linux/swapops.h
+++ b/msm-google/include/linux/swapops.h
@@ -409,6 +409,22 @@ static inline pte_t swp_entry_and_counter_to_pte(swp_entry_t entry, u64 counter)
 	return __swp_entry_to_pte(arch_entry);
 }
 
+static inline bool pte_to_swp_excepted(pte_t pte)
+{
+	swp_entry_t arch_entry;
+	arch_entry = __pte_to_swp_entry(pte);
+	return __swp_excepted(arch_entry);
+}
+
+static inline pte_t swp_entry_with_excepted(swp_entry_t entry)
+{
+	swp_entry_t arch_entry;
+	arch_entry = __swp_entry_with_excepted(swp_type(entry), swp_offset(entry), 0UL, 1UL);
+	return __swp_entry_to_pte(arch_entry);
+}
+
+
+
 
 #endif
 
diff --git a/msm-google/include/trace/events/mmflags.h b/msm-google/include/trace/events/mmflags.h
index 40b9cc3bf..4b42e7aff 100644
--- a/msm-google/include/trace/events/mmflags.h
+++ b/msm-google/include/trace/events/mmflags.h
@@ -80,6 +80,10 @@
 #define IF_HAVE_PG_IDLE(flag,string)
 #endif
 
+#ifdef CONFIG_APP_AWARE
+#define IF_HAVE_PG_EXCEPTED(flag,string) ,{1UL << flag, string}
+#endif
+
 #define __def_pageflag_names						\
 	{1UL << PG_locked,		"locked"	},		\
 	{1UL << PG_waiters,		"waiters"	},		\
@@ -106,7 +110,8 @@ IF_HAVE_PG_MLOCK(PG_mlocked,		"mlocked"	)		\
 IF_HAVE_PG_UNCACHED(PG_uncached,	"uncached"	)		\
 IF_HAVE_PG_HWPOISON(PG_hwpoison,	"hwpoison"	)		\
 IF_HAVE_PG_IDLE(PG_young,		"young"		)		\
-IF_HAVE_PG_IDLE(PG_idle,		"idle"		)
+IF_HAVE_PG_IDLE(PG_idle,		"idle"		)      \
+IF_HAVE_PG_EXCEPTED(PG_excepted,	"excepted"	)
 
 #define show_page_flags(flags)						\
 	(flags) ? __print_flags(flags, "|",				\
diff --git a/msm-google/jyp/remote.c b/msm-google/jyp/remote.c
index 88a895feb..2b3699d19 100644
--- a/msm-google/jyp/remote.c
+++ b/msm-google/jyp/remote.c
@@ -64,10 +64,12 @@ struct per_app_swap_trace *past[9];
 int backgrounded_uid;
 atomic_t sent_cold_page;
 atomic_t faulted_cold_page;
+atomic_t excepted_page;
 
 int prefetch_on;
 int target_percentage;
-int random_nbd_entry;
+
+struct task_struct *preempted_cold_task;
 
 struct perapp_cluster pac[10];
 
@@ -158,12 +160,9 @@ static int send_zram_to_nbd(unsigned int id, pte_t *pte, pmd_t *pmd, unsigned lo
 	SetPageUnevictable(page);
 					
 	//printk(KERN_ERR "[REMOTE %s] alloc page %llx\n", __func__,(unsigned long)page_address(page));
-	if(random_nbd_entry)
-		new_entry = get_swap_page_of_type(NBD_TYPE);
-	else
 		new_entry = get_swap_page_of_id(id);
-	new_pte = swp_entry_to_pte(new_entry);
-	//if Prefetch target, type==1 && counter==0
+	new_pte = swp_entry_and_counter_to_pte(new_entry,id);
+	//if Prefetch target, type==1 && counter(id)==id
 	lock_page(page);
 	__SetPageSwapBacked(page);
 	ClearPageUptodate(page);
@@ -205,7 +204,7 @@ static int send_zram_to_nbd(unsigned int id, pte_t *pte, pmd_t *pmd, unsigned lo
 	}
 	
 	set_pte(orig_pte, new_pte);
-	trace_printk("target sent: %d %llx %llx\n",mm->owner->tgid, vpage, swp_offset(new_entry));
+	trace_printk("target sent id %d: %d %llx %llx\n",id, mm->owner->tgid, vpage, swp_offset(new_entry));
 	swap_free(entry);
 	ret=1;
 unlock:
@@ -287,7 +286,7 @@ static void cold_page_sender_work(struct work_struct *work)
 						continue;
 					}
 					
-					if(swp_swapcount(entry) != 1 || counter <= COLD_PAGE_THRESHOLD) {
+					if(swp_type(entry) == NBD_TYPE || swp_swapcount(entry) != 1 || counter <= COLD_PAGE_THRESHOLD) {
 						continue;
 					}
 					else                    // page is swapped and swap entry.
@@ -311,13 +310,10 @@ static void cold_page_sender_work(struct work_struct *work)
 						
 					//	printk(KERN_ERR "[REMOTE %s] alloc page %llx\n", __func__,(unsigned long)page_address(page));
 
-						if(random_nbd_entry)
-							new_entry = get_swap_page_of_type(NBD_TYPE);
-						else
 							new_entry = get_swap_page_of_id(9); // cold page
 
-						new_pte = swp_entry_and_counter_to_pte(new_entry,1);
-						//if COLD, type==1 && counter==1
+						new_pte = swp_entry_and_counter_to_pte(new_entry,9);
+						//if COLD, type==1 && counter(id)==9 (cold id)
 
 
 						
@@ -377,6 +373,13 @@ static void cold_page_sender_work(struct work_struct *work)
 //						printk(KERN_ERR "[REMOTE %s] free page %llx\n", __func__,(unsigned long)page_address(page));
 						free_page((unsigned long)(page_address(page)));
 
+						if(switch_start){
+							trace_printk(KERN_ERR "[REMOTE %s] switch started during cold work. preempted! \n", __func__);
+							atomic_add(cnt, &sent_cold_page);
+							kfree(tew);
+							preempted_cold_task = task ;
+							return;
+						}
 						
 					}
 				}
@@ -390,6 +393,8 @@ static void cold_page_sender_work(struct work_struct *work)
 	trace_printk("remote: total faulted cold page: %d\n", faulted_cold_page);
 	kfree(tew);
 
+	preempted_cold_task = NULL;
+
 }
 
 
@@ -408,7 +413,18 @@ static int fault_target_page(pid_t tgid, unsigned long va){
 	rcu_read_lock();
 	task = find_task_by_vpid(tgid);
 	rcu_read_unlock();
-	
+
+
+	if(!task)     
+    {
+	//	printk(KERN_ERR "[REMOTE %s] task is NULL \n", __func__);
+        return 0;
+    }
+    
+        
+	if(task->mm && task->mm->mmap) 
+    {
+
 	mm = task->mm ;
 	vma = find_vma(mm,va);
 	pgd = pgd_offset(mm, va);
@@ -472,6 +488,8 @@ static int fault_target_page(pid_t tgid, unsigned long va){
 		}
 	}       
 
+
+	}
 	return 0;
 }
 
@@ -496,7 +514,19 @@ static int prefetch_target_page(pid_t tgid, unsigned long va){
 	rcu_read_lock();
 	task = find_task_by_vpid(tgid);
 	rcu_read_unlock();
-	
+
+	if(!task)     
+    {
+	//	printk(KERN_ERR "[REMOTE %s] task is NULL \n", __func__);
+        return 0;
+    }
+    
+        
+	if(task->mm && task->mm->mmap) 
+    {
+
+
+
 	mm = task->mm ;
 	vma = find_vma(mm,va);
 	pgd = pgd_offset(mm, va);
@@ -550,6 +580,11 @@ static int prefetch_target_page(pid_t tgid, unsigned long va){
 		}
 	}       
 
+
+
+	}
+
+
 	return 0;
 }
 
@@ -814,9 +849,6 @@ int ksg_handler(struct ctl_table *table, int write,
                                     pte_t new_pte;
 									int ret;
 
-									if(random_nbd_entry)
-										new_entry = get_swap_page_of_type(NBD_TYPE);
-									else
 										new_entry = get_swap_page_of_id(id);
 									
                                     new_pte = swp_entry_to_pte(new_entry);
@@ -1006,8 +1038,6 @@ int app_switch_start;
 int app_switch_start_handler(struct ctl_table *table, int write,
 			   void __user *buffer, size_t *length, loff_t *ppos)
 {
-
-	
 	int rc = proc_dointvec_minmax(table,write,buffer,length,ppos);
 	unsigned int id;
 	unsigned long flags;
@@ -1030,10 +1060,6 @@ int app_switch_start_handler(struct ctl_table *table, int write,
 		if(foreground_uid && prefetch_on)
 			prefetch_handler(id,target_table);
 
-		spin_lock_irqsave(&switch_start_lock,flags);
-		switch_start = 1;
-		spin_unlock_irqrestore(&switch_start_lock,flags);
-
 		
 		spin_lock_irqsave(&which_table_lock,flags);
 
@@ -1051,8 +1077,9 @@ int app_switch_start_handler(struct ctl_table *table, int write,
 			atomic_set(&past[id]->st_index0,-1);
 
 
-
-
+		spin_lock_irqsave(&switch_start_lock,flags);
+		switch_start = 1;
+		spin_unlock_irqrestore(&switch_start_lock,flags);
 
 	}
 
@@ -1163,7 +1190,7 @@ int app_switch_fin_handler(struct ctl_table *table, int write,
 
 
 		if(foreground_uid && atomic_read(&past[id]->st_index0)!=-1 && atomic_read(&past[id]->st_index1)!=-1){
-			update_to_nbd_flag(id, target_percentage); //<--app_number or uid
+			update_to_nbd_flag(id, 100-target_percentage); //<--app_number or uid
 			past[id]->st_should_check = 1 ; // --> per app, and keep in list
 		}
 
@@ -1180,6 +1207,11 @@ int app_switch_fin_handler(struct ctl_table *table, int write,
 					printk("backgrounded_uid %d",backgrounded_uid);
 					if(task_swap_counter_inc(p))
 						cold_page_sender_handler(p);
+					/* for preempted task */
+					if(preempted_cold_task){
+						trace_printk("preempted cold task recheck\n");
+						cold_page_sender_handler(preempted_cold_task);
+					}
 				}
 			}
 			rcu_read_unlock();
@@ -1291,6 +1323,7 @@ int swap_counter_dump_handler(struct ctl_table *table, int write,
 
 	trace_printk("remote: total sent cold page: %d\n", sent_cold_page);
 	trace_printk("remote: total faulted cold page: %d\n", faulted_cold_page);
+	trace_printk("remote: total excepted page: %d\n", excepted_page);
 	}
 	return 0;
 }
@@ -1328,7 +1361,7 @@ static int send_target_page(unsigned int id, pid_t tgid, unsigned long va){
 
 	if(!task)     
     {
-		printk(KERN_ERR "[REMOTE %s] task is NULL \n", __func__);
+		//printk(KERN_ERR "[REMOTE %s] task is NULL \n", __func__);
         return 0;
     }
 
@@ -1574,6 +1607,8 @@ static int __init remote_swap_init(void)
 		init_past(past[i]);
 	}
 
+	preempted_cold_task = NULL;
+
 	error = send_target_managers_init();
 	if (unlikely(error))
 		return error;
diff --git a/msm-google/kernel/sysctl.c b/msm-google/kernel/sysctl.c
index c2b2737f1..d3471324b 100644
--- a/msm-google/kernel/sysctl.c
+++ b/msm-google/kernel/sysctl.c
@@ -436,13 +436,6 @@ static struct ctl_table kern_table[] = {
 		.mode			= 0644,
 		.proc_handler	= proc_dointvec,
 	},
-	{
-		.procname		= "random_nbd_entry",
-		.data			= &random_nbd_entry,
-		.maxlen			= sizeof(unsigned int),
-		.mode			= 0644,
-		.proc_handler	= proc_dointvec,
-	},
 #endif
 #if defined(CONFIG_PREEMPT_TRACER) || defined(CONFIG_DEBUG_PREEMPT)
 	{
diff --git a/msm-google/mm/memory.c b/msm-google/mm/memory.c
index c821993eb..8dc43c912 100644
--- a/msm-google/mm/memory.c
+++ b/msm-google/mm/memory.c
@@ -2924,9 +2924,12 @@ int do_swap_page(struct vm_fault *vmf)
 #ifdef CONFIG_APP_AWARE
 
 	int idx;
-	unsigned int id;
+	unsigned int id = 0;
 	atomic_t *st_idx_ptr;
 	struct swap_trace_entry *swap_trace_table;
+	bool excepted = 0;
+	if(switch_start && foreground_uid)
+		id = get_id_from_uid(foreground_uid);
 
 #endif
 
@@ -3025,11 +3028,25 @@ int do_swap_page(struct vm_fault *vmf)
 		 *
 		*/
 		if (swp_type(entry) == NBD_TYPE){
-			if(pte_to_swp_counter(vmf->orig_pte)){
+			if(pte_to_swp_counter(vmf->orig_pte)==9) { //cold
 				atomic_inc(&faulted_cold_page);
+				atomic_dec(&sent_cold_page);
+			}
+			else if(switch_start && id && pte_to_swp_counter(vmf->orig_pte) == id){  // fault page: switch start, and sent page get fault
+					trace_printk("prefetch fault id %d: %d \"%s\" %lx %lx\n",get_id_from_uid(foreground_uid),current->tgid,current->comm,vmf->address,swp_offset(entry));
 			}
 			else{
-				trace_printk("prefetch fault %d \"%s\" %lx %lx\n",current->tgid,current->comm,vmf->address,swp_offset(entry));
+				trace_printk("Exception : %d \"%s\" %lx %lx\n",current->tgid,current->comm,vmf->address,swp_offset(entry));
+				atomic_inc(&excepted_page);
+				SetPageExcepted(page);
+				excepted = 1;
+			}
+		}
+		else{ // ZRAM_TYPE
+			if(pte_to_swp_excepted(vmf->orig_pte)){
+				trace_printk("Exception touched after marked : %d \"%s\" %lx %lx\n",current->tgid,current->comm,vmf->address,swp_offset(entry));
+				SetPageExcepted(page);
+				excepted = 1;
 			}
 		}
 #endif
@@ -3107,8 +3124,7 @@ int do_swap_page(struct vm_fault *vmf)
 
 #ifdef CONFIG_APP_AWARE
 
-	if(switch_start && foreground_uid){
-		id = get_id_from_uid(foreground_uid);
+	if(switch_start && foreground_uid && !excepted){
 		if(past[id]->which_table){
 			st_idx_ptr = &past[id]->st_index1;
 			swap_trace_table = past[id]->swap_trace_table1;
diff --git a/msm-google/mm/rmap.c b/msm-google/mm/rmap.c
index cc4bb882d..056aea100 100644
--- a/msm-google/mm/rmap.c
+++ b/msm-google/mm/rmap.c
@@ -1644,7 +1644,12 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 			}
 			dec_mm_counter(mm, MM_ANONPAGES);
 			inc_mm_counter(mm, MM_SWAPENTS);
+
 			swp_pte = swp_entry_to_pte(entry);
+#ifdef CONFIG_APP_AWARE
+			if(swp_type(entry)==ZRAM_TYPE && PageExcepted(page))
+				swp_pte = swp_entry_with_excepted(entry);
+#endif
 			if (pte_soft_dirty(pteval))
 				swp_pte = pte_swp_mksoft_dirty(swp_pte);
 			set_pte_at(mm, address, pvmw.pte, swp_pte);
diff --git a/msm-google/mm/swap_state.c b/msm-google/mm/swap_state.c
index cd793663b..b373e18cb 100644
--- a/msm-google/mm/swap_state.c
+++ b/msm-google/mm/swap_state.c
@@ -295,6 +295,11 @@ int add_to_swap(struct page *page)
 	if (!entry.val)
 		return 0;
 
+#ifdef CONFIG_APP_AWARE
+	if(swp_type(entry)==NBD_TYPE)
+		trace_printk("ZRAM full: nbd swapout offset %llx",swp_offset(entry));
+#endif
+
 	if (mem_cgroup_try_charge_swap(page, entry))
 		goto fail;
 
@@ -474,10 +479,10 @@ struct page *lookup_swap_cache(swp_entry_t entry, struct vm_area_struct *vma,
 #ifdef CONFIG_APP_AWARE
 			if (swp_type(entry) == NBD_TYPE){
 				if(switch_start)
-					trace_printk("prefetch hit %d \"%s\" %lx %lx\n",current->tgid,current->comm,addr,swp_offset(entry));
+					trace_printk("prefetch hit id %d: %d \"%s\" %lx %lx\n",get_id_from_uid(foreground_uid),current->tgid,current->comm,addr,swp_offset(entry));
 				else
 				{
-					trace_printk("prefetch miss %d \"%s\" %lx %lx\n",current->tgid,current->comm,addr,swp_offset(entry));
+					trace_printk("prefetch miss id %d: %d \"%s\" %lx %lx\n",get_id_from_uid(foreground_uid),current->tgid,current->comm,addr,swp_offset(entry));
 
 
 				}
